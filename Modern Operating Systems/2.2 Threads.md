# Threads

Each process has an address space and a single thread of control. Nevertheless it is desiderable to have multiple threads of control in the same address space running in quasi-parallel as if they were (almost) separate processes (except for the shared address space).

Threads are faster (10-100 times) to create and destroy compared to processes. Morevoer threads are useful on systems with multiple CPUs where real parallelism is possible.

**Example 1.** Word processor

If a word processor were single threaded then whenever a disk backup started, commands from the keyboard and mouse would be ignored until the backup was finished. We can think of a thread that interacts with the user, a second that reformats the document when asked to and a third that writes content from ram to disk periodically. Having three different processes here would not work because they all need to operate on the document.

**Example 2.** Web server

A dispatcher thread to read incoming requests from the network and worker threads to handle the requests.

There exists an alternative to simulate threads and their stacks the hard way: finite state machine. 

Model | Characteristic
--- | ---
Threads | Parallelism, blocking system calls
Single thread system process | No parallelism, blocking system calls
Finite state machine | Parallelism, non blockig system calls, interrupts


## Classical thread model

Processes are used to group resources together. Threads are the entities scheduled for execution on the CPU.

When a multithreaded process is run on a single CPU, the threads take turns running.

**Remark.** Different threads in a process are not as independent as different processes. All threads have the same address space, which means they also share the same global variables.
One thread can read, write or even wipe out another thread's stack. There is no protection mechanism between threads because it is impossible and it should not be necessary. Unlike different processes, which may be from different users, a process is always owned by a single user.

Per-process item | Per-thread item
--- | ---
Address space | Program counter
Global variablles | Registers
Open files | Stack
Child processes | State
Pending alarms | 
Signals and signals handlers | 
Accounting information | 

## POSIX threads

Thread call | Description
--- | ---
`pthread_create` | Create a new thread
`pthread_exit` | Terminate the calling thread
`pthread_join` | Wait for a specific thread to exit
`pthread_yield` | Release the CPU to let another thread run
`pthread_attr_init` | Create and initialize a thread's attribute structure

The call `pthread_create` is very much like `fork` (except with parameters), with the thread identifier playing the role of the PID.
Sometimes a thread is not logically blocked but it feels that it has run long enough and wants to give another thread a chance to run.

## Implementing threads

There are two main places to implement threads: the user space and the kernel. Hybrid implementations are also possible.

### Implementing in user space

When threads are implemented in user space each process needs its own private thread table to keep track of the threads in that process.

When a thread does something that may cause it to become blocked locally it calls a run time system procedure.
This procedure checks to see if the thread must be put in blocked state.

Doing thread switching like this is at least *an order of magnitude faster* than trapping to the kernel.
User level threads have also the advantage that *each process can have its own customized scheduling*.

**Disadvantages**

How blocking system calls are implemented? One of the main goals of having threads in the first place is to allow each one to make blocking system calls but to prevent one blocked thread from affecting the others. With blocking system calls how can this be achieved?

The system calls could all be changed to be non blocking (eg. a read on the keyboard would just return 0 bytes if no characters were already buffered) but requiring changes to the OS is unattractive.

Another alternative is available in the event that is possible to tell if advance if a call will block. The system call `select` allows the caller to tell whether a prospective `read` will block. If the `read` call will block than the call is not made and another thread is run. The next time the run time system gets control it can check again if `read` is again blocking. This approach requires rewriting part of the system call library: it is inefficient and inelegant but there is little choice. The code placed around the system call is called *jacket* or *wrapper*.

Another problem in some way analogous to blocking system calls is the one of *page faults*.
Not all of a program is set in memory at once. If the program calls or jumps to an instruction that is not in memory a page fault will occur and the OS will get the missing instructions (and its neighbours) from the disk. This is called a page fault. If a thread causes a page fault, the kernel, unaware of even the existence of threads, blocks the entire process until the disk I/O is complete, even though other threads might be runnable.

Another problem is that if a thread starts running, no other thread in the process will ever run unless the first thread volountarily gives up to the CPU.

### Implementing in the kernel

The kernel has a thread table that keeps track of all the threads in the sytem. 

Due to the relatively greater cost of creating and destroying threads in the kernel some systems take an environmentally correct approach and recycle their threads. When a thread is destroyed is marked as non runnable but its kernel data structures are not affected. Later when a new created must be created an old thread is reactivated saving some overhead.

Thread recyling is also possible for user level threads but there is no incentive in doing this since the thread management overhead is much smaller.

**Problems.**

What happens when a multithreaded process forks? Does the new process have as many threads as the old one or just one?

Another issue is signals. Signals are sent to processes not to threads. When a signal comes in, which thread should handle it?

### Scheduler activations

The goals of the scheduler activation work are to mimic the functionality of kernel threads but with better performance and greater flexibility usually associated with threads implemented in user space.

Efficiency is achieved by avoiding unnecessary transitions between user and kernel space.

When scheduler activations are used the kernel assigns a certain number of virtual processors to each process and lets the (user space) run-time system allocate threads to processors.

When the kernel knows that a thread has blocked (eg. by having executed a blocking system call or caused a page fault), the kernel notifies the process' run time system, passing as parameter on the stack the number of the thread and a description of the event. This mechanism is called an **upcall**.
Once activated the run time system can reschedule its threads.

Cons: the reliance on upcalls violates the structure inherent in any layered system. Layer *n* offers certain services that layer *n+1* can call on but layer *n* may not call procedures in layer *n+1*.

### Pop-up threads

Let's consider the example of how incoming messages are handled. The traditional approach is to have a process or thread that is blocked on a `receive` system call waiting for an incoming message.

A different approach is possible: the arrival of a message causes the system to create a new thread to handle the message, called a **pop-up thread**.

Advantages: since pop-up threads are brand new they do not have any history (registers,stack,etc.) that must be restored. This makes it possible to create a thread quickly, with short latency between message arrival and start of processing.

Some advance planning is needed: in which process does the thread run? Having a pop-up thread run in kernel space is easier and faster than putting it in user space. Also a pop up thread in kernel space can access all the kernel's tables and I/O devices which may be needed for interrupt processing.
On the other hand a buggy kernel thread can do more damage than a buggy user thread.

### Making single threaded code multithreaded

**Problem 1.**

Consider the *errno* variable maintained by Unix. When a process (or thread) makes a system call that fails, the error code is put into *errno*.

Imagine thread 1 executes the system call `access` to find out if it has permission to access a certain file. The OS returns the answer in the global variable *errno*. After control has returned to thread 1 but before it has a chance to read *errno* the scheduler switches to thread 2.
Thread 2 executes an `open` call that fails making *errno* overwritten. When thread 1 starts up later it will read the wrong value.

A solution is to assign to each thread its own private global variables.

**Problem 2.**

Many libraries procedures are not reentrant meaning that they were not designed to have a second call made to any given procedure while a previous call has not yiet finished.

A solution is to provide each procedure with a jacket to mark if the libray is in use. Any attempt for another thread to use a library procedure while a previous call has not yet completed is blocked. This approach works but eliminates potential parallelism.

**Problem 3.**

Signals. When thread sare implemented entirely in user space the kernel does not even know about threads and can hardly direct the signal to the right one.

**Problem 4.**

Stack management. When a process stack overflows the kernel provides the process with more stack automatically. When a  process has multiple threads it must have multiple stacks.

