## InterProcess Communication

In IPC there are 3 issues: 
* how can one process pass information to another
* how to make sure two processes do not get into each other way
* proper sequencing when dependencies are present

### Race Conditions

In order to avoid race conditions we need **mutual exclusion**.

The part of the program where the shared memory is accessed is called *critical region* or *critical section*. A requirement to avoid race conditions is that no two processes are in their critical region at the same time. However this condition itself is not sufficient for having parallel processes cooperate correctly and efficiently. We need the following:

1. No two processes may be simultaneously inside their critical regions
2. No assumption may be made about speeds or the number of CPUs
3. No process running outside its critical region may block any process 
4. No process should have to wait forever to enter its critical region

#### Mutual exclusion by busy waiting

**Disable interrupts**

On a single processor system the easiest solution is to have each process disable all interrupts after entering its critical region and re-enable them just before leaving it.

This approach is unattractive since it is unwise to give user processes the power to turn off interrupts. What if one process does not turn them on? It would be the end of the system.

**Lock variables**

We could consider a shared (lock) variable set to 0. If the lock is 0 the process sets it to 1 and enters its critical region. If the lock is already 1 the process waits until it becomes 0.

Unfortunately there is a flaw: suppose one process reads the lock and sees it is 0. Before it can set the lock to 1 another process is scheduled, runs and sets the lock to 1.

**Strict alternation**

```
while (TRUE) {
	while (turn != 0) critical_region();

	turn = 1;
	noncritical_region();
}
```

Continuously testing a variable until some value appears is called **busy waiting**. It should be avoided since it wastes CPU time.
A lock that uses busy waiting is called a **spin lock**.

Taking turns is not a good idea when one of the two processes is much slower than the other.

This situation violates condition 3 set above.

**Peterson's solution**

```
#define FALSE 0
#define TRUE  1
#define N	  2

int turn;
int interested[N];

void enter_region(int process) {
	int other;
	other = 1-process;
	interested[process] = TRUE;
	turn = process;
	while (turn == process && interested[other] == TRUE) /* null statement */ ;
}

void leave_region(int process) {
	interested[process] = FALSE;
}
```
Before using the shared variables (eg. before entering its critical region) each process calles `enter_region` with its own process number as parameter. This call will cause it to wait until it is safe to enter.

**The TSL instruction**

`TSL RX, LOCK`

The TSL (Test and Set Lock) instruction works as follows: it reads the content of the memory word *lock* into register `RX` and then stores a non zero value at the memory address lock. The operations of reading the word and storing into it are **guaranteed to be indivisible**. 

It is important to note that locking the memory bus is very different from disabling the interrupts. Disabling interrupts then performing a read on memory word followed by a write does not prevent a second processor on the bus from accessing the word between the read and write. In fact disabling interrupts on processor 1 has no effect at all on processor 2.

```
enter_region:
	TSL REGISTER,LOCK		// copy lock to register and set lock to 1
	CMP REGISTER,#0			// was lock zero?
	JNE enter_region		// if it was not zero lock was set, so loop
	RET

leave_region:
	MOVE LOCK,#0
	RET
```

An alternative instruction to TSL is XCHG which exchanges the content of two locations atomically.

### Sleep and Wakeup

Both Peterson's solution and the ones using TSL or XCHG are correct, but both have the defect of requiring busy waiting. In essence what these solutions do is this: when a process wants to enter its critical region, it checks to see if the entry is allowed. If it is not the process just sits in a tight loop waiting until it is.

Not only this approach wastes CPU time but also leads to unexpected results. Let us have two processes H and L, with high and low priorities respectively. When L is in its critical region, H becomes ready to run. H begins busy waiting but since L is never scheduled while H is running, L never gets a chance to leave its critical region, so H loops forever. This is sometimes referred to as the **priority inversion problem**.

**The produced consumer problem**

Two processes shared a common, fixed size buffer. One of the them, the producer, puts information inside the buffer, and the other one, the consumer, takes it out.

Trouble arises when the consumer wants to insert a new item in the buffer but it is already full or when the consumer wants to retrive an item but the buffer is empty. The solution is for the producer to go to sleep and be awakened when the consumer has removed at least one item and for the consumer to go to sleep until the producer has put a new item in the buffer.

This approach is simple but incurs in a race condition. We need a variable *count* to keep track of the number of items in the buffer.

```
#define N 100
int count = 0;

void producer(void)
{
	int item;

	while (TRUE)
	{
		item = produce_new_item();
		if (count == N) sleep();
		insert_item(item);
		count = count +1;
		if (count == 1) wakeup(consumer);
	}
}

void consumer(void)
{
	int item;

	while (TRUE)
	{
		if (count == 0) sleep();
		item = remove_item();
		count = count-1;
		if (count == N-1) wakeup(producer);
		consumer_item(item);
	}
}
```

It could happen that the consumer has just read *count* to see if it is 0.
The scheduler stops the consumer temporarily and start running the producer. The producer inserts an element and increment *count* which now is 1 and thus sends a *wakeup* to the consumer.
Since the consumer is not logically asleep the *wakeup* is lost. When the consumer next runs it tests the value of *count* previously read, find it to be 0 and goes to sleep. Sooner or later the producer will fill the buffer and go to sleep. Both will sleep forever.

The essence of the problem is that *a wakeup that is sent to a non sleeping process is lost*.

A quick fix is to add a wakeup waiting bit to the picture. It saves the day but becomes insufficient with more complex examples.

### Semaphores

Dijkstra suggested using an integer variable, called **semaphore**, to count the number of wakeups saved for future use.

There are two operations on semaphores: *up* and *down*. *down* checks if the value of the semaphore is greater than 0. If so it decrements the value. If the value is 0 the process is put to sleep without completing the down.

Checking the value, changing it and possibly going to sleep are all done as a **single indivisible atomic action**.

The *up* operation increments the value of the semaphore addressed. If one or more processes were sleeping on that semaphore, unable to complete an earlier *down*, one of them is chosen by the system and is allowed to complete its *down*.

#### Solving the producer-consumer problem with semaphores

```
#define N 100
typedef int semaphore;
semaphore mutex = 1; // make sure the producer and the consumer do not access the buffer at the same time
semaphore empty = N; // number of slots that are empty
semaphore full = 0;  // number of slots that are full

void producer(void)
{
	int item;

	while (TRUE)
	{
		item = produce_new_item();
		down(&empty);				// decrement empty count
		down(&mutex);				// enter critical region
		insert_item(item);
		up(&mutex);					// leave critical region
		up(&full);					// increment count of full slots
	}
}

void consumer(void)
{
	int item;

	while (TRUE)
	{
		down(&full);
		down(&mutex);
		item = remove_item();
		up(&mutex);
		up(&empty);
		consume_item(item);
	}
}
```

**Remark.** The *mutex* semaphore is used for **mutual exclusion**. It is designed to guarantee that only a process at time will write or read a certain buffer. The *full* and *empty* semaphores are used for **synchronization**, they guarantee that certain event sequences do or do not occurr.

### Mutexes

When the semaphore's ability to count is not needed, a simplified version of the semaphore, called mutex, is used.
Mutexes are used for managing mutual exclusion to some shared resources or piece of code.

A mutex is a shared variable that can be only in two states: lock or unlocked.

With a user threads package there is no problem with multiple threads having access to the same mutex. However with Peterson's solution there is an unspoken assumption that multiple processes have access to at least some shared memory. What if the processes have disjoint address spaces?

There are two options: some of the shared data structures, such as the semaphores, can be stored in the kernel and accessed by system calls. In alternative most OS offer a way for processes to share some portion of their address space with other processes.

### Futexes

Spin locks (check a lock by busy waiting) are fast if the wait is short but waste CPU cycles if it is not. If there is much contention it is more efficient to block the process and let the kernel unblock it only when the lock is free. This has inverse problem: it works well under heavy contention but continuously switching to the kernel is expensive if there is little contention.

A solution that tries to combine the best of both is known as **futex** (fast userspace mutex).
A futex implements basic locking (much like a mutex) but avoids dropping into the kernel unless it really has to. A futex consists of two parts: a kernel service and a user library. The kernel service provides a "wait queue" that allows multiple processes to wait on a lock. They will not run unless the kernel explicitly unblocks them.

When a thread inspects the lock, if it is not in the locked state all is well and our thread has grabbed the lock. However if the lock is held by another thread our thread has to wait. In that case the futex library does not spin but uses a system call to put the thread in the wait queue. Hopefully the cost of the switch to the kernel is now justified because the thread was blocked anyway.

### Monitors

With semaphores and mutexes IPC looks easy right? Forget it. Suppose that the two *down* in the producer consumer's problem were swapped. Both processes would stay blocked forever. This situation is called a deadlock.
With semaphores one subtle error makes everything go to halt. There is an alternative to write code easier: monitors.

A monitor is a collection of procedures, variables and data structures that are all grouped together in a special kind of module or package. Only one process can be active in a monitor at any instant.

It is up to the compiler to implement mutual exclusion on monitor entries but a common way is to use a mutex or a binary semaphore. Because the compiler, not the programmer, is arranging for mutual exclusion, it is much less likely that something will go wrong.

For the developer it is sufficient to know that by turning all the critical regions into monitors procedures no two processes will ever execute their critical region at the same time.

**Remark.** Monitors provide an easy way to achieve mutual exclusion but it is not enough: we need a way for processes to block when they can not proceed. The solution lies in the introduction of *condition variables* along with two operations on them, *wait* and *signal*.

To avoid having two active processes in the monitor at the same time we need a rule telling what happens after a signal.
First proposal: the newly awakened process is in charge of suspending the other one.
Second proposal: a process doing a signal must exit the monitor immediately. In other words a signal statement may appear only as the final statement in a monitor procedure.

We stick to the second proposal because it is conceptually simpler.

**Remark.** Condition variables are not counters. They do not accumulate signals for later use the way semaphores do. Thus if a condition variable is signaled with no one to waiting on it the signal is lost forever. The wait must come before the signal.

Monitors require a language that has them built it.

### Message Passing

A problem with monitors and also with semaphores is that they were designed for solving the mutual exclusion problem on one or more CPU that have access to a common shared memory. By putting the semaphores in the shared memory and protecting them with TSL or XCHG instructions we can avoid races. When we move to a distributed system consisting of multiple CPUs, each with its own private memory and connected by a LAN, these primitives become inapplicable. We need message passing.

### Design issues

* Messages can be lost on the network -> use an acknwoledge message 
* The ACK message can be lost on the network. It is essential that the sender can distinguish a new message from the retransmission of an old one -> consecutive sequence numbers in each original message
* How processes are named
* Authentication
* Performance: if the sender and the receiver are on the same machine copying a message from a process to another is slower than doing a semaphore operation or entering a monitor.

### The producer consumer problem with message passing

```
#define N 100

void produce(void)
{
	int item;
	message m;

	while (TRUE)
	{
		item = produce_item();
		receive(consumer, &m);
		build_message(&m, item);
		send(consumer, &m);
	}

	void consumer(void)
	{
		int itemm i;
		message m;

		for (i=0; i>N; i++)
		{
			send(producer, &m);
		}

		while (TRUE)
		{
			receive(producer, &m);
			item = extract_item(&m);
			send(producer, &m);
			consumer_item(item);
		}
	}
}
```

If the producer works faster than the consumer all the messages will end up full, waiting for the consumer. The producer will become blocked waiting for an empty to come back.

## Scheduling

When a computer is multiprogrammed it has multiple processes or threads competing for the CPU at the same time. The part of the OS that decides which process runs next is called the scheduler. In addition to picking the right process to run the scheduler has also to worry about making efficient use of the CPU because process switching is expensive.

Processes that spend most of their time computing are called CPU-bound while processes that spend most of their time waiting for I/O are I/O bound.

When to schedule:

* when a new process is created
* when a process exits
* when a process blocks (on I/O, on a semaphore, etc.)
* when an I/O interrupts occurs

**Nonpreemptive** scheduling algorithms pick a process to run and let it run until it blocks.
**Preemptive** scheduling algorithms pick a process and let it run for a maximum of some fixed time.

Categories of scheduling algorithms:

* Batch 
* Interactive
* Real time

### Goals

All systems:

* Fairness: give each process a fair share of the CPU
* Policy enforcement: seeing that stated policy is carried out
* Balance: keeping all parts of the system busy

Batch systems:

* Throughput: maximize jobs per hour
* Turnaround time: minimize time between submission and termination
* CPU utilization: keep the CPU busy at all the time

Interactive systems:

* Response time: respond to requests quickly
* Proportionality: meet user expectations

Real time systems:

* Meeting deadlines: avoid losing data
* Predictability: avoid quality degrdation in multimedia systems

### Scheduling in batch systems

#### First come, first served

Processes are assigned the CPU in the order they requested it. There is a single queue of ready processes. When a blocking process becomes ready it is put at the end of the queue, behind all waiting processes.

Disadvantage: suppose there is one compute bound process that runs for 1 sec at time and many I/O bound processes that use little CPU but each has to perform 1000 reads from disk.
The result is that each I/O bound process gets to read 1 block per second and will take 1000 seconds to finish. With a scheduling algorithm that preempted the compute bound process every 10msec the I/O bound processes would finish in 10 sec.

#### Shortest job first

Given the assumption that run times are known in advance consider four processes a,b,c,d. The first finishes at time a, the second at time a+b, the third a+b+c and so on. The mean turnaround time is (4a+3b+2c+d)/4. It is clear that a contributes more to the average so it should be the shortest job.

This algorithm is optimal only when all the jobs are available simultaneously.

#### Shortest remaining time next

A preemptive version of the shortest job first is shortest remaining time next.

The scheduler always chooses the process whose remaining run time is shortest.

### Scheduling in interactive systems

#### Round robin scheduling

Each process is assigned a time interval, called its quantum, during which it is allowed to run. If the process is still running before the end of the quantum the CPU is preempted and given to another process.

When the process uses up its quantum it is put at the end of the list.

The interesting issue with round robin is the length of the quantum. Switching from one process to another (process switch or context switch) requires a certain amount of time for saving and loading registers and memory maps, updating various tables and lists, flushing and reloading the memory cache and so on.

Setting the quantum too short causes too many process switches and lowers the CPU efficiency, but setting it too long may cause poor response time to short interactive requests. A quantum around 20-50msec is often a good compromise.

#### Priority scheduling

Round robin makes the implicit assumption that all processes are equally important.

Priorities can be assigned to processes statically or dynamicaly.

A simple algorithm for giving good service to I/O bound processes is to set the priority to 1/f where f is the fraction of the last quantum a process used. A process that used only 1 msec of its 50msec quantum would get priority 50 while a process that run 25 msec before blocking would get priority 2.

It is often convenient to group processes in priority classes and use priority scheduling among the classes but round robin scheduling within each class.

#### Lottery scheduling

The idea is to give processes lottery tickets for different resources. Whenever a scheduling decision is made a lottery ticket is chosen at random and the process holding the ticket gets the resource.

More important processes cna be given extra tickets to increase their odds of winning. 

Lottery scheduling has several interesting processes. If a new process shows up and is granted some tickets at the next lottery it will have a chance of winning that is proportional to the number of tickets it holds. In oter words lottery scheduling is higly resposive.

Cooperating processes may exchange tickets: when a client process sends a message to a server process and then blocks it may give all of its ticket to the server to increase the chance of the server to be running next. When the server replies it returns the tickets.

Another example is a video server in which several processes are feeding video streams to their clients but at different rates, e.g. 10, 20 and 25 frame/sec. By allocating these processes 10, 20 and 25 tickets respectively they will automatically divide the CPU in the correct proportion.

#### Other algorithms

* Multiple queues
* Shortest process next
* Guaranteed scheduling

#### Policy versus Mechanism

A process may have many children running under its control. The process may have an excellent idea of which of its children are the most important and which the least. However none of the schedulers discussed so far accepts any input from user processes about scheduling decisions.

The solution to this problem is to separate the **scheduling mechanism** from the **scheduling policy**, meaning that the scheduling algorithm is parameterized in some way but the parameters can be filled in by user processes.

### Thread scheduling

Case 1: user level threads

The kernel is not aware of the existence of threads and it operates as it always does. Given two processes A and B with thread A1,A2,A3 and B1,B2,B3 a possible scheduling could be

A1,A2,A3,A1,A2,A3

while A1,B1,A2,B2,A3,B3 would not be possible.

Note that there is no clock to interrupt a thread that has run too long but since threads cooperate this is not an issue.

Case 2: kernel level threads

In this case a scheduling sequence like A1,B1,A2,B2,A3,B3 is possible.